{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691770dc-9388-4727-a3ed-7dfa9e795290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc48c1-5ec7-4abf-8659-25cd3c477103",
   "metadata": {},
   "source": [
    "# Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05854a4c-a053-4d90-bf64-8faecbe7681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for dair-ai/emotion contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/dair-ai/emotion\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d875e4bd-8cd5-4166-8efb-ee654350bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "label_mapping = {0:\"sadness\", 1:\"joy\", 2:\"love\", 3:\"anger\", 4:\"fear\", 5:\"surprise\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5baa6e-64a0-4cf1-9255-d4cf50cb4db2",
   "metadata": {},
   "source": [
    "## Exploring The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e2969cd-04c6-42eb-98d8-cb7a644409ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ca0d5-a70b-467f-a56c-ed7ceb797c6f",
   "metadata": {},
   "source": [
    "# Loading The Model And Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657a3aed-4d7b-4b5d-b252-dd8822b215f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5929aa33-cfa3-496c-aaab-222baad74726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ea741fc3d64aa7a1d14047b396393f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize The Data\n",
    "tokenized_datasets = {}\n",
    "\n",
    "for split in dataset.keys():\n",
    "    tokenized_datasets[split] = dataset[split].map(lambda x: tokenizer(x['text'], truncation=True, padding=\"max_length\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1f5b82c-f89d-455a-88d5-ce1cd42d210a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 16000\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 2000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 2000\n",
       " })}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2009ab25-376d-4509-95a5-e4359e8cdfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bc460a35674515949e110d14dadeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load The Model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6,\n",
    "                                 id2label=label_mapping)\n",
    "\n",
    "# freeze Model Parameters\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef8a50-1fb0-413f-a991-a55492c2e543",
   "metadata": {},
   "source": [
    "## Exploring The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf3891a0-2666-46ad-91fc-d0ebf351f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69c210-859a-4da7-a9a2-ae48ee4ded3c",
   "metadata": {},
   "source": [
    "# PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae3e9bf0-a87d-4f46-ba4d-85995d110438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 10:19:10.490438: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-18 10:19:10.716838: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb90387-0400-4760-ab7b-a189327239ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd5446eb-8f19-49b7-a641-26df3d323f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def training(model, tokenizer, datasets, compute_metrics):\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./data',\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=2e-5,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch')\n",
    "    \n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[\"train\"],\n",
    "        eval_dataset=datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b545f09-9ad0-4c48-89f9-b6795997f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lora Config File\n",
    "config = LoraConfig(r=10, target_modules=['q_lin', 'k_lin', 'v_lin', 'lin1', 'lin2'], \n",
    "                    lora_alpha=16, lora_dropout=0.1, bias=\"none\", \n",
    "                    task_type=TaskType.SEQ_CLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5f66fb6-20ab-4f98-be8d-7899910a3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model With PEFT Config File\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a797d9e6-0644-438d-af9a-c3c45f0360ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,332,486 || all params: 68,290,572 || trainable%: 1.9512005258939698\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8cc483-359f-49b3-b5ac-f1301274997f",
   "metadata": {},
   "source": [
    "## Evaluate Model Prior To FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2ae4261-6297-45c6-9416-a3e794f14ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = training(model, tokenizer, tokenized_datasets, compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da47746d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 25:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.701501</td>\n",
       "      <td>0.743500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.477719</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.429400</td>\n",
       "      <td>0.386786</td>\n",
       "      <td>0.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.345996</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.334149</td>\n",
       "      <td>0.879000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.5932028839111328, metrics={'train_runtime': 1549.9086, 'train_samples_per_second': 51.616, 'train_steps_per_second': 3.226, 'total_flos': 1.09256196096e+16, 'train_loss': 0.5932028839111328, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Without Finetuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1809fd4f-8ed4-43a2-982a-663fb70bc19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3298896849155426,\n",
       " 'eval_accuracy': 0.8915,\n",
       " 'eval_runtime': 15.0157,\n",
       " 'eval_samples_per_second': 133.194,\n",
       " 'eval_steps_per_second': 8.325,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base Model Evaluations\n",
    "trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ba0b26d-42c5-43e3-9d33-e286b41fe200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling quite sad and sorry for myself but ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like i am still looking at a blank canv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel like a faithful servant</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am just feeling cranky and blue</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i can have for a treat or if i am feeling festive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>i feel like i ve been having some issues with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>im feeling more fucked up than last night</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>i can feel violent biff whole length is hit by...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>i am reading something the saints have written...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>im not trying to sound sarcastic but only tryi...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label  predicted_label\n",
       "0   im feeling quite sad and sorry for myself but ...      0                0\n",
       "1   i feel like i am still looking at a blank canv...      0                0\n",
       "2                      i feel like a faithful servant      2                2\n",
       "3                   i am just feeling cranky and blue      3                3\n",
       "4   i can have for a treat or if i am feeling festive      1                1\n",
       "..                                                ...    ...              ...\n",
       "95  i feel like i ve been having some issues with ...      1                0\n",
       "96          im feeling more fucked up than last night      3                3\n",
       "97  i can feel violent biff whole length is hit by...      3                3\n",
       "98  i am reading something the saints have written...      2                2\n",
       "99  im not trying to sound sarcastic but only tryi...      0                3\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tokenized_datasets[\"validation\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa14a3-d407-4f7c-a058-ab2ea5e60938",
   "metadata": {},
   "source": [
    "## Train PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22925028-d70a-4f22-9904-4b507962d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer = training(lora_model, tokenizer, tokenized_datasets, compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0d958a3-365f-4ae8-bea5-706f03d901de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 25:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.266441</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.250300</td>\n",
       "      <td>0.246760</td>\n",
       "      <td>0.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.234768</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>0.227305</td>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.223209</td>\n",
       "      <td>0.907000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/checkpoint-4000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/checkpoint-5000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.2510279769897461, metrics={'train_runtime': 1553.8289, 'train_samples_per_second': 51.486, 'train_steps_per_second': 3.218, 'total_flos': 1.09256196096e+16, 'train_loss': 0.2510279769897461, 'epoch': 5.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f5f39b4-8aec-411c-8fda-fca8211afeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60ef4574-6eba-406c-8e2a-b7ce94b33eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2f1f99b-3c9a-40bb-9957-f1b97f799a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.21582446992397308,\n",
       " 'eval_accuracy': 0.925,\n",
       " 'eval_runtime': 14.7934,\n",
       " 'eval_samples_per_second': 135.196,\n",
       " 'eval_steps_per_second': 8.45,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2a2dd50-a8ed-408c-b88c-6ef33ff6f120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling quite sad and sorry for myself but ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i feel like i am still looking at a blank canv...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i feel like a faithful servant</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am just feeling cranky and blue</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i can have for a treat or if i am feeling festive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>i feel like i ve been having some issues with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>im feeling more fucked up than last night</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>i can feel violent biff whole length is hit by...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>i am reading something the saints have written...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>im not trying to sound sarcastic but only tryi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label  predicted_label\n",
       "0   im feeling quite sad and sorry for myself but ...      0                0\n",
       "1   i feel like i am still looking at a blank canv...      0                0\n",
       "2                      i feel like a faithful servant      2                2\n",
       "3                   i am just feeling cranky and blue      3                3\n",
       "4   i can have for a treat or if i am feeling festive      1                1\n",
       "..                                                ...    ...              ...\n",
       "95  i feel like i ve been having some issues with ...      1                1\n",
       "96          im feeling more fucked up than last night      3                3\n",
       "97  i can feel violent biff whole length is hit by...      3                3\n",
       "98  i am reading something the saints have written...      2                2\n",
       "99  im not trying to sound sarcastic but only tryi...      0                0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tokenized_datasets[\"validation\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "predictions = peft_trainer.predict(tokenized_datasets[\"validation\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc828445-2432-4a28-81dd-f32eb7376bf0",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fd08b0b-09fa-4b6b-9c02-17aa2c3b68aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "\n",
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\"lora_model\",  num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bf4964a-12d7-4c46-9186-56c76d5e546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'I can\\'t believe I saw a shark eat that seagull at the beach today!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "957668c4-d480-4482-ac5d-c1eaf430f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'I hate when the bread gets soggy in my sandwiches.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a187b42-3872-46c9-98dc-3d066b915549",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = 'I love having eggs on toast for breakfast, especially when the yolks are runny.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ee980dd-8dcc-42de-8639-fe0ef1d91684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inf(text):\n",
    "    input = tokenizer(text, \n",
    "                  padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    logits = inference_model(**input).logits\n",
    "    predictions = torch.argmax(logits,dim=1).numpy()[0]\n",
    "\n",
    "    print(f'Tweet: {text}')\n",
    "    print()\n",
    "    print(f'Prediction: {predictions} ({label_mapping[predictions]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0eae36e3-c040-4415-89ad-14ab44dcb5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I can't believe I saw a shark eat that seagull at the beach today!\n",
      "\n",
      "Prediction: 5 (surprise)\n"
     ]
    }
   ],
   "source": [
    "run_inf(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14e8d68a-b0c8-4dda-a631-ed5fd0f3c178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I hate when the bread gets soggy in my sandwiches.\n",
      "\n",
      "Prediction: 0 (sadness)\n"
     ]
    }
   ],
   "source": [
    "run_inf(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7ee28cd-9409-440b-ad47-88462581b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: I love having eggs on toast for breakfast, especially when the yolks are runny.\n",
      "\n",
      "Prediction: 3 (anger)\n"
     ]
    }
   ],
   "source": [
    "run_inf(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb8dd2-b7fc-4c3f-856d-9416fc3cb034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
